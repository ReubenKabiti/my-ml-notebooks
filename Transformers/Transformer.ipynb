{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTXnBTc-Uwjf",
        "outputId": "38cd5801-8b22-4cdb-e09d-36905e1eb932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-27 12:00:23--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-12-27 12:00:24 (79.8 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "RO27tFd1VUv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_length = 10000\n",
        "with open(\"input.txt\", \"r\") as file:\n",
        "  corpus = file.read()[:corpus_length]\n"
      ],
      "metadata": {
        "id": "OK5yO6J5fi_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EOS = \"<eos>\""
      ],
      "metadata": {
        "id": "icUZ7oMGl69y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data loading"
      ],
      "metadata": {
        "id": "vr-vKnVBs7mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = corpus.split('\\n')\n",
        "lines = list(filter(lambda x: len(x) > 0, lines))\n",
        "examples = []\n",
        "for line in lines:\n",
        "  words = line.split()\n",
        "  examples.extend([\" \".join(words[:i] + [EOS]) for i in range(1, len(words) - 1)])\n",
        "\n",
        "word_index = {word: i for i, word in enumerate(set(\" \".join(examples).split()))}\n",
        "index_word = {i: word for i, word in enumerate(set(\" \".join(examples).split()))}\n",
        "maximum_sequence_length = len(max(examples))\n",
        "num_words = len(word_index)"
      ],
      "metadata": {
        "id": "InlF280FfuFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## helper functions for conversions"
      ],
      "metadata": {
        "id": "5Uk4cd4wsuf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def texts2seqs(texts, word_index, maximum_sequence_length):\n",
        "  seqs = []\n",
        "  for text in texts:\n",
        "    words = text.split()\n",
        "    seq = [word_index.get(word) for word in words]\n",
        "    for i, s in enumerate(seq):\n",
        "      if s is None:\n",
        "        seq[i] = word_index[EOS]\n",
        "      else:\n",
        "        seq[i] = s\n",
        "    len_seq = len(seq)\n",
        "    seq = seq + [word_index[EOS]] * (maximum_sequence_length - len_seq)\n",
        "    seqs.append(seq)\n",
        "  return torch.tensor(seqs)\n",
        "\n",
        "def seqs2texts(seqs, index_word):\n",
        "  texts = []\n",
        "  for seq in seqs:\n",
        "    words = []\n",
        "    for s in seq:\n",
        "      word = index_word.get(s.item())\n",
        "      if word is None:\n",
        "        word = EOS\n",
        "      if word == EOS:\n",
        "        words.append(word)\n",
        "        break\n",
        "      words.append(word)\n",
        "    text = ' '.join(words)\n",
        "    texts.append(text)\n",
        "  return texts\n",
        "\n",
        "def seqs2onehot(seqs, num_classes):\n",
        "  one_hots = []\n",
        "  for seq in seqs:\n",
        "    one_hots.append(\n",
        "            F.one_hot(seq, num_classes=num_classes)\n",
        "            .type(torch.float32)\n",
        "            .numpy()\n",
        "        )\n",
        "  return torch.tensor(one_hots)\n",
        "\n",
        "def dense2seqs(dense):\n",
        "  seqs = []\n",
        "  B, T, C = dense.shape\n",
        "  for b in range(B):\n",
        "    denses = dense[b]\n",
        "    indices = torch.multinomial(denses, num_samples=1).squeeze(1)\n",
        "    seqs.append(indices)\n",
        "  return seqs"
      ],
      "metadata": {
        "id": "Upo7x18wiOFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset preparation"
      ],
      "metadata": {
        "id": "0AKBl5kbtEFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "LY9cZRRpoi-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, examples, word_index, maximum_sequence_length):\n",
        "    self.examples = examples\n",
        "    self.word_index = word_index\n",
        "    self.maximum_sequence_length = maximum_sequence_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    example = self.examples[idx]\n",
        "    seqs = texts2seqs([example], self.word_index, self.maximum_sequence_length)[0]\n",
        "    return seqs\n",
        "\n",
        "dataset = MyDataset(examples, word_index, maximum_sequence_length)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "jQ1yVu7ntNag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model architecture"
      ],
      "metadata": {
        "id": "ZUk2JtfauaeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn"
      ],
      "metadata": {
        "id": "f7apMCwsuR2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.layer = nn.Sequential(\n",
        "        nn.Linear(embedding_dim, embedding_dim),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, embedding_dim, query_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.query_dim = query_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.w_qs = nn.ParameterList([\n",
        "            nn.Parameter(torch.randn(embedding_dim, query_dim)) for _ in range(num_heads)\n",
        "        ])\n",
        "    self.w_ks = nn.ParameterList([\n",
        "            nn.Parameter(torch.randn(embedding_dim, query_dim)) for _ in range(num_heads)\n",
        "        ])\n",
        "    self.w_vs = nn.ParameterList([\n",
        "            nn.Parameter(torch.randn(embedding_dim, query_dim)) for _ in range(num_heads)\n",
        "        ])\n",
        "    self.w = nn.Parameter(torch.randn(num_heads * query_dim, embedding_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    zs = []\n",
        "    for i, (w_q, w_k, w_v) in enumerate(zip(self.w_qs, self.w_ks, self.w_vs)):\n",
        "      q = x @ w_q\n",
        "      k = x @ w_k\n",
        "      v = x @ w_v\n",
        "      z = F.softmax(q@k.transpose(1, 2)/(self.embedding_dim**0.5), dim=1) @ v\n",
        "      zs.append(z)\n",
        "    zs = torch.cat(zs, dim=2)\n",
        "    return zs @ self.w\n",
        "\n",
        "class TrainablePositionalEncoding(nn.Module):\n",
        "  def __init__(self, num_positions, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(num_positions, embedding_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.emb(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, embedding_dim, query_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.multihead_attention = MultiheadAttention(embedding_dim, query_dim, num_heads)\n",
        "    self.ffn = FFN(embedding_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    y = self.multihead_attention(x)\n",
        "    x = F.layer_norm(x + y, normalized_shape=(B, T, C))\n",
        "\n",
        "    y = self.ffn(x)\n",
        "    return F.layer_norm(x + y, normalized_shape=(B, T, C))\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, num_positions, num_words, embedding_dim, query_dim, num_heads, num_blocks=6):\n",
        "    super().__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.query_dim = query_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.positional_encoding = TrainablePositionalEncoding(num_positions, embedding_dim)\n",
        "    self.embedding = nn.Embedding(num_words, embedding_dim)\n",
        "    self.blocks = nn.ParameterList([Block(embedding_dim, query_dim, num_heads) for _ in range(num_blocks)])\n",
        "    self.last_layer = nn.Sequential(\n",
        "        nn.Linear(embedding_dim, embedding_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(embedding_dim, num_words),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T = x.shape\n",
        "    x = self.embedding(x)\n",
        "    x = x + self.positional_encoding(torch.arange(T, device=x.device))\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "    x = self.last_layer(x)\n",
        "    x = F.softmax(x, dim=2)\n",
        "    return x\n",
        "\n",
        "  def summary(self):\n",
        "    # Count the number of parameters\n",
        "    num_params = sum(p.numel() for p in self.parameters())\n",
        "    print(f\"Total number of parameters: {num_params}\")\n",
        "\n",
        "    # Count only trainable parameters\n",
        "    num_trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "    print(f\"Number of trainable parameters: {num_trainable_params}\")\n",
        "\n",
        "\n",
        "embedding_dim = 512\n",
        "query_dim = 64\n",
        "num_heads = 6\n",
        "num_blocks=32\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "__zna6y1urKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(model, start_word, num_words):\n",
        "  model.eval()\n",
        "  start_word = texts2seqs([start_word], word_index, maximum_sequence_length)[:, :1]\n",
        "  start_word = start_word.to(device)\n",
        "  with torch.no_grad():\n",
        "    for _ in range(num_words):\n",
        "      logits = model(start_word)\n",
        "      logits = logits[:, -1, :].unsqueeze(1)\n",
        "      seqs = dense2seqs(logits)[0]\n",
        "      start_word = torch.cat([start_word, seqs.unsqueeze(0)], dim=1)\n",
        "  texts = seqs2texts(start_word, index_word)\n",
        "  return texts"
      ],
      "metadata": {
        "id": "Dag8f1Gk1nXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(\n",
        "      maximum_sequence_length,\n",
        "      num_words, embedding_dim,\n",
        "      query_dim, num_heads, num_blocks\n",
        "    ).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "mmX2RIuX8em2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "num_epochs = 100\n",
        "lr = 3e-4\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0\n",
        "  for batch, x in enumerate(loader):\n",
        "    x = x.to(device)\n",
        "    y = model(x)\n",
        "    y_target = seqs2onehot(x.cpu(), num_words).to(device)\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(y, y_target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "    if batch % 9 == 0:\n",
        "      print(f\"Epoch: {epoch}, Batch: {batch}, Loss: {total_loss/10}\")\n",
        "      total_loss = 0\n"
      ],
      "metadata": {
        "id": "ZgAJv_Ge9_aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer(model, 'almost', 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "2Z1Dh6nz_kTc",
        "outputId": "f6cb8650-e4a9-4082-e8b3-8b5686ae8b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-255-64d8d2337ae4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'almost'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-209-30471f6964d2>\u001b[0m in \u001b[0;36minfer\u001b[0;34m(model, start_word, num_words)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mstart_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts2seqs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximum_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mstart_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model_state.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "hIAiOEayHlDC",
        "outputId": "8c00181f-6446-401c-b537-5dd67229db8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-256-6b6f0fac4765>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_state.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;31m# .cpu() on the underlying Storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m                 \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;34m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    }
  ]
}